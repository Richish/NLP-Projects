{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BasicTextPreprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJv+MjoAxuPxiUkPkryUQx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richish/NLP-Projects/blob/main/BasicTextPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZzykGro-hrg"
      },
      "source": [
        "# Basic Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_elvD2f-tEq"
      },
      "source": [
        "### Tokenizers Using nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbubiViQ-Zmp",
        "outputId": "6c8c5c2d-a65a-4f41-9560-98e231ad10f4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "texts = \"\"\"This is great. This is a sentence. Colab google: uploading images in multiple subdirectories: If you would like to upload images (or files) in multiples subdirectories by using Colab google, please follow the following steps: - I'll suppose that your images(files) are split into 3 subdirectories (train, validate, test) in the main directory called (dataDir): 1- Zip the folder (dataDir) to (dataDir.zip) 2- Write this code in a Colab cell:\"\"\"\n",
        "\n",
        "for sentence in sent_tokenize(texts):\n",
        "    print(sentence)\n",
        "    print(word_tokenize(sentence))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "This is great.\n",
            "['This', 'is', 'great', '.']\n",
            "This is a sentence.\n",
            "['This', 'is', 'a', 'sentence', '.']\n",
            "Colab google: uploading images in multiple subdirectories: If you would like to upload images (or files) in multiples subdirectories by using Colab google, please follow the following steps: - I'll suppose that your images(files) are split into 3 subdirectories (train, validate, test) in the main directory called (dataDir): 1- Zip the folder (dataDir) to (dataDir.zip) 2- Write this code in a Colab cell:\n",
            "['Colab', 'google', ':', 'uploading', 'images', 'in', 'multiple', 'subdirectories', ':', 'If', 'you', 'would', 'like', 'to', 'upload', 'images', '(', 'or', 'files', ')', 'in', 'multiples', 'subdirectories', 'by', 'using', 'Colab', 'google', ',', 'please', 'follow', 'the', 'following', 'steps', ':', '-', 'I', \"'ll\", 'suppose', 'that', 'your', 'images', '(', 'files', ')', 'are', 'split', 'into', '3', 'subdirectories', '(', 'train', ',', 'validate', ',', 'test', ')', 'in', 'the', 'main', 'directory', 'called', '(', 'dataDir', ')', ':', '1-', 'Zip', 'the', 'folder', '(', 'dataDir', ')', 'to', '(', 'dataDir.zip', ')', '2-', 'Write', 'this', 'code', 'in', 'a', 'Colab', 'cell', ':']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj0-zFgEAtIw"
      },
      "source": [
        "### Removing stop words, punctuations and digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQvxpqmD_8mv",
        "outputId": "46e0054d-aee9-4e70-b7c4-6f09254df318"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "nltk.download('stopwords')\n",
        "texts = [texts]\n",
        "def preproces_corpus(texts):\n",
        "    my_stopwords = set(stopwords.words('english'))\n",
        "    def remove_stops_digits(tokens):\n",
        "        return [token.lower() for token in tokens if token not in my_stopwords and token not in punctuation and not token.isdigit()]\n",
        "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
        "\n",
        "print(preproces_corpus(texts))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[['this', 'great', 'this', 'sentence', 'colab', 'google', 'uploading', 'images', 'multiple', 'subdirectories', 'if', 'would', 'like', 'upload', 'images', 'files', 'multiples', 'subdirectories', 'using', 'colab', 'google', 'please', 'follow', 'following', 'steps', 'i', \"'ll\", 'suppose', 'images', 'files', 'split', 'subdirectories', 'train', 'validate', 'test', 'main', 'directory', 'called', 'datadir', '1-', 'zip', 'folder', 'datadir', 'datadir.zip', '2-', 'write', 'code', 'colab', 'cell']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrwEGzsVUOWU"
      },
      "source": [
        "### Stemmming and Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUAjw1J9UYnq",
        "outputId": "e0ac6f00-40a6-4481-bde7-b143d2128087"
      },
      "source": [
        "# stemming using nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "word1, word2 = \"footballs\", \"authorization\"\n",
        "print(stemmer.stem(word1), stemmer.stem(word2))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "footbal author\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryz78965UY0U",
        "outputId": "83822e5d-c34d-4f4a-8086-b2a663e358c8"
      },
      "source": [
        "# lemmatization using nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"authorization\", pos=\"a\"), lemmatizer.lemmatize(\"better\", pos=\"a\"))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "authorization good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RwL7W7DUY3K",
        "outputId": "91856757-5f73-4906-fc5e-61cb201c711c"
      },
      "source": [
        "# lemmatization using spacy\n",
        "import spacy\n",
        "sp = spacy.load(\"en_core_web_sm\")\n",
        "token = sp(\"better\")\n",
        "for word in token:\n",
        "    print(word.text, word.lemma_)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "better well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dP6CLfkWxLI"
      },
      "source": [
        "## some advanced preprocessing like pos tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eXsE7w5UY5z",
        "outputId": "6901f714-7837-4a45-801b-be52607ec665"
      },
      "source": [
        "# using spacy\n",
        "import spacy\n",
        "sp = spacy.load(\"en_core_web_sm\")\n",
        "doc = sp(\"Kimaya was born to Arshdeep kaur and harvinder singh (village khurd) in 2001 during winter month of december\")\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.shape_, token.is_alpha, token.is_stop)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kimaya Kimaya PROPN Xxxxx True False\n",
            "was be AUX xxx True True\n",
            "born bear VERB xxxx True False\n",
            "to to ADP xx True True\n",
            "Arshdeep Arshdeep PROPN Xxxxx True False\n",
            "kaur kaur PROPN xxxx True False\n",
            "and and CCONJ xxx True True\n",
            "harvinder harvinder NOUN xxxx True False\n",
            "singh singh NOUN xxxx True False\n",
            "( ( PUNCT ( False False\n",
            "village village NOUN xxxx True False\n",
            "khurd khurd PROPN xxxx True False\n",
            ") ) PUNCT ) False False\n",
            "in in ADP xx True True\n",
            "2001 2001 NUM dddd False False\n",
            "during during ADP xxxx True True\n",
            "winter winter NOUN xxxx True False\n",
            "month month NOUN xxxx True False\n",
            "of of ADP xx True True\n",
            "december december PROPN xxxx True False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQjIIrqgUY80"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fe1FCm1UZA8"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}